{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbf0df0",
   "metadata": {},
   "source": [
    "# Data collection for Optimsation Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff1dbe3",
   "metadata": {},
   "source": [
    "Before running code, pvlib will need to be installed in the terminal. This is done with the comand:\n",
    "pip install pvlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d5ab8e",
   "metadata": {},
   "source": [
    "\n",
    "Import necessary modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a00410ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pvlib\n",
    "from pvlib import pvsystem\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import math\n",
    "import glob\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3dbc79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Blank cells detected in the *Input Data* tab (first row):\n",
      "\n",
      "  → Column 'Available South Facing Roof Area for Panels (m2)' is blank\n",
      "  → Column 'Available East Facing Roof Area for Panels (m2)' is blank\n",
      "  → Column 'Available West Facing Roof Area for Panels (m2)' is blank\n",
      "Blank cells set to zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/openpyxl/worksheet/_read_only.py:85: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n",
      "/opt/anaconda3/lib/python3.12/site-packages/openpyxl/worksheet/_read_only.py:85: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "%run ./variables.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b7cd90",
   "metadata": {},
   "source": [
    "Configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1671e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#site_coords = (-35.7333, 143.9167)  # East Kerang coordinates (lat, lon)\n",
    "#start_year = 2023\n",
    "end_year = start_year\n",
    "start_month = 1\n",
    "peak_hours = [(15, 21)]  # Peak period: 3pm-9pm\n",
    "pv_type = 'roof_flush'  # or 'roof_tilt'\n",
    "\n",
    "# Spot market configuration\n",
    "#nem_state = 'VIC1'  # VIC1, NSW1, QLD1, SA1, TAS1\n",
    "download_spot_prices = True\n",
    "\n",
    "# # Electricity Bill Charges (monthly)\n",
    "# elec_bill_charges = [\n",
    "#     0.66528+0.6456+1.15714+0.0294+0.55125+10.62,      # Jan\n",
    "#     0.64476+0.5556+1.15714+0.01262+0.05438+10.62,     # Feb\n",
    "#     0.75768+0.8504+1.14330+0.01405+0.04326+0.00987+9.81,  # Mar\n",
    "#     0.75768+0.8504+1.14330+0.01663+0.04364+9.81,      # Apr\n",
    "#     0.75768+0.8504+1.14330+0.08053+0.04343+9.81,      # May\n",
    "#     0.75768+0.8504+1.14330+0.01790+0.04333+9.81,      # Jun\n",
    "#     0.75768+0.8504+1.14330+0.01460+0.05510+10.62,     # Jul\n",
    "#     0.75768+0.8504+1.14330+0.02245+0.05561+10.62,     # Aug\n",
    "#     0.75768+0.8504+1.14330+0.02018+0.05587+10.62,     # Sep\n",
    "#     0.75768+0.8504+1.14330+0.02450+0.05635+10.62,     # Oct\n",
    "#     0.75768+0.8504+1.14330+0.02282+0.05588+10.62,     # Nov\n",
    "#     0.75768+0.8504+1.14330+0.02319+0.05575+0.11172+10.62  # Dec\n",
    "# ]\n",
    "\n",
    "# Setup output directory structure\n",
    "base_dir = Path.cwd()\n",
    "data_dir = base_dir / 'data'\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create year-specific folder\n",
    "if start_year == end_year:\n",
    "    year_dir = data_dir / str(start_year)\n",
    "else:\n",
    "    year_dir = data_dir / f\"{start_year}_{end_year}\"\n",
    "year_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30a0ce",
   "metadata": {},
   "source": [
    "#### File analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5ef703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(filepath):\n",
    "    \"\"\"Check if file exists and return boolean.\"\"\"\n",
    "    exists = Path(filepath).exists()\n",
    "    if exists:\n",
    "        print(f\"File already exists: {filepath}\")\n",
    "    return exists\n",
    "\n",
    "\n",
    "def resample_to_5min(df, timestamp_col='Datetime'):\n",
    "    \"\"\"Resample DataFrame to 5-minute intervals using forward fill.\"\"\"\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "    df = df.drop_duplicates(subset=[timestamp_col], keep='first').set_index(timestamp_col)\n",
    "\n",
    "    inferred_freq = pd.infer_freq(df.index)\n",
    "    if inferred_freq in ['5min', '5T']:\n",
    "        return df.reset_index()\n",
    "\n",
    "    print(f\"Detected frequency: {inferred_freq}. Resampling to 5min...\") \n",
    "\n",
    "    # Extend the index by one more expected period (assuming hourly)\n",
    "    last_time = df.index[-1]\n",
    "    next_time = last_time + pd.Timedelta(hours=1)\n",
    "    df_extended = pd.concat([df, pd.DataFrame(index=[next_time])])\n",
    "\n",
    "    df_resampled = df_extended.resample('5min').ffill()\n",
    "    \n",
    "    # Drop the final (midnight) row and restore original column name\n",
    "    df_resampled = df_resampled.iloc[:-1].reset_index()\n",
    "    df_resampled.rename(columns={'index': timestamp_col}, inplace=True)\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "# def resample_to_5min_kw(df, timestamp_col, data_cols):\n",
    "#     \"\"\"\n",
    "#     Resample hourly kW data to 5-minute intervals by dividing by 12.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     df : pd.DataFrame\n",
    "#         Input dataframe with hourly data\n",
    "#     timestamp_col : str\n",
    "#         Name of timestamp column\n",
    "#     data_cols : list\n",
    "#         List of column names to resample\n",
    "#     \"\"\"\n",
    "#     df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "#     print(\"LOOK HERRRREE\")\n",
    "#     print(df[[timestamp_col]].head(5))\n",
    "#     data_5min = []\n",
    "    \n",
    "#     for _, row in df.iterrows():\n",
    "#         timestamp = row[timestamp_col]\n",
    "#         per_5min_values = {col: row[col] / 12 for col in data_cols}\n",
    "        \n",
    "#         for i in range(12):\n",
    "#             new_timestamp = timestamp + pd.Timedelta(minutes=5*i)\n",
    "#             new_row = {'timestamp': new_timestamp}\n",
    "#             new_row.update(per_5min_values)\n",
    "#             data_5min.append(new_row)\n",
    "    \n",
    "#     df_5min = pd.DataFrame(data_5min)\n",
    "#     df_5min['timestamp'] = df_5min['timestamp'].dt.strftime('%-d/%-m/%Y %-H:%M')\n",
    "#     return df_5min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c1eb4",
   "metadata": {},
   "source": [
    "Resample dataframe 5 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c615c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_5min_kw(df, timestamp_col, data_cols):\n",
    "    \"\"\"\n",
    "    Resample hourly kW data to 5-minute intervals by dividing by 12.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with hourly data\n",
    "    timestamp_col : str\n",
    "        Name of timestamp column\n",
    "    data_cols : list or str\n",
    "        List of column names to resample, or single column name as string\n",
    "    \"\"\"\n",
    "    # Convert single string to list\n",
    "    if isinstance(data_cols, str):\n",
    "        data_cols = [data_cols]\n",
    "    \n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "    print(df[[timestamp_col]].head(5))\n",
    "    data_5min = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        timestamp = row[timestamp_col]\n",
    "        per_5min_values = {col: row[col] / 12 for col in data_cols}\n",
    "        \n",
    "        for i in range(12):\n",
    "            new_timestamp = timestamp + pd.Timedelta(minutes=5*i)\n",
    "            new_row = {'timestamp': new_timestamp}\n",
    "            new_row.update(per_5min_values)\n",
    "            data_5min.append(new_row)\n",
    "    \n",
    "    df_5min = pd.DataFrame(data_5min)\n",
    "    df_5min['timestamp'] = df_5min['timestamp'].dt.strftime('%-d/%-m/%Y %-H:%M')\n",
    "    return df_5min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4446b",
   "metadata": {},
   "source": [
    "#### Grid/Retail Price Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62de6ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_prices(prices, peak_hours, start_year, end_year, start_month=1):\n",
    "    \"\"\"Generate hourly grid prices with peak/off-peak rates.\"\"\"\n",
    "    if start_year == end_year:\n",
    "        filename = year_dir / f\"grid_prices_{start_year}.csv\"\n",
    "    else:\n",
    "        filename = year_dir / f\"grid_prices_{start_year}_{end_year}.csv\"\n",
    "    \n",
    "    if file_exists(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        first_month = start_month if year == start_year else 1\n",
    "        start_date = f\"{year}-{first_month:02d}-01 00:00\"\n",
    "        end_date = pd.Period(f\"{year}-12\", freq=\"M\").end_time.replace(hour=23)\n",
    "        \n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq=\"h\")\n",
    "        \n",
    "        hourly_prices = []\n",
    "        for dt in date_range:\n",
    "            month_idx = dt.month - 1\n",
    "            peak_price, offpeak_price = prices[month_idx]\n",
    "            is_peak = any(start <= dt.hour < end for start, end in peak_hours)\n",
    "            hourly_prices.append(peak_price if is_peak else offpeak_price)\n",
    "        \n",
    "        year_df = pd.DataFrame({\n",
    "            \"Datetime\": date_range,\n",
    "            \"Price ($/kWh)\": hourly_prices\n",
    "        })\n",
    "        all_data.append(year_df)\n",
    "    \n",
    "    df = pd.concat(all_data, ignore_index=True)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Created: {filename}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# test_retail = generate_grid_prices(prices, peak_hours, start_year, end_year, start_month=1)\n",
    "\n",
    "# test_retail = resample_to_5min(test_retail)\n",
    "\n",
    "# print(test_retail.tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d968c66",
   "metadata": {},
   "source": [
    "#### Solar Generation Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "311bb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data(lat, lon, year, start_year=2005, end_year=2023):\n",
    "    \"\"\"Fetch TMY weather data from PVGIS.\"\"\"\n",
    "    all_data = pvlib.iotools.get_pvgis_tmy(\n",
    "        float(lat), float(lon),\n",
    "        startyear=start_year, endyear=end_year,\n",
    "        outputformat='json', map_variables=True\n",
    "    )\n",
    "    \n",
    "    data = all_data[0].reset_index()\n",
    "    data['time(UTC)'] = pd.to_datetime(data['time(UTC)'])\n",
    "    data['time(UTC)'] = data['time(UTC)'].apply(lambda x: x.replace(year=year))\n",
    "    data = data.set_index('time(UTC)')\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_pv_profile(weather_data, latitude, longitude, surface_azimuth, \n",
    "                        surface_tilt, type_pv='roof_flush'):\n",
    "    \"\"\"Calculate PV generation profile (kW per kW installed).\"\"\"\n",
    "    temperature = weather_data['temp_air'].astype(float)\n",
    "    windspeed = weather_data['wind_speed'].astype(float)\n",
    "    ghi = weather_data['ghi']\n",
    "    dni = weather_data['dni']\n",
    "    dhi = weather_data['dhi']\n",
    "    pressure = weather_data['pressure'].astype(float)\n",
    "    \n",
    "    sp = pvlib.solarposition.get_solarposition(\n",
    "        weather_data.index, float(latitude), float(longitude), pressure=pressure\n",
    "    )\n",
    "    \n",
    "    irradiance = pvlib.irradiance.get_total_irradiance(\n",
    "        surface_tilt, surface_azimuth, sp.apparent_zenith, sp.azimuth,\n",
    "        dni, ghi, dhi\n",
    "    )\n",
    "    \n",
    "    modules = pvsystem.retrieve_sam('SandiaMod').T\n",
    "    modules_multi_si = modules[modules['Material'] == 'mc-Si']\n",
    "    multi_si = modules_multi_si[\n",
    "        modules_multi_si.index.str.contains('Sharp_NDQ235F4__2013_')\n",
    "    ].T.iloc[:, 0]\n",
    "    \n",
    "    if type_pv == 'roof_flush':\n",
    "        temp_model_params = pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS['sapm']['close_mount_glass_glass']\n",
    "    elif type_pv == 'roof_tilt':\n",
    "        temp_model_params = pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS['sapm']['open_rack_glass_polymer']\n",
    "    else:\n",
    "        raise ValueError('Invalid type of PV panel installation')\n",
    "    \n",
    "    celltemp = pvlib.temperature.sapm_cell(\n",
    "        irradiance.poa_global, temperature, windspeed, **temp_model_params\n",
    "    )\n",
    "    \n",
    "    airmass = pvlib.atmosphere.get_relative_airmass(sp.apparent_zenith)\n",
    "    airmass_absolute = pvlib.atmosphere.get_absolute_airmass(airmass, pressure=pressure)\n",
    "    aoi = pvlib.irradiance.aoi(surface_tilt, surface_azimuth, sp.zenith, sp.azimuth)\n",
    "    \n",
    "    effective_irradiance = pvlib.pvsystem.sapm_effective_irradiance(\n",
    "        irradiance.poa_direct, irradiance.poa_diffuse,\n",
    "        airmass_absolute, aoi, multi_si\n",
    "    )\n",
    "    \n",
    "    dc = pvlib.pvsystem.sapm(effective_irradiance, celltemp, multi_si)\n",
    "    kWp = (multi_si.Impo * multi_si.Vmpo) / 1000\n",
    "    \n",
    "    sapm_inverters = pvlib.pvsystem.retrieve_sam('cecinverter')\n",
    "    inverter = sapm_inverters['ABB__MICRO_0_25_I_OUTD_US_208__208V_']\n",
    "    ac = pvlib.inverter.sandia(dc['v_mp'], dc['p_mp'], inverter)\n",
    "    \n",
    "    ac_mod = ac.fillna(0) / 1000\n",
    "    ac_mod[ac_mod < 0] = 0\n",
    "    profile = (1/kWp) * ac_mod\n",
    "    \n",
    "    return profile\n",
    "\n",
    "\n",
    "def generate_directional_solar_profiles(lat, lon, start_year, end_year, type_pv='roof_flush'):\n",
    "    \"\"\"Generate solar profiles for all four cardinal directions.\"\"\"\n",
    "    if start_year == end_year:\n",
    "        filename = year_dir / f\"solar_hourly_generation_{start_year}.csv\"\n",
    "    else:\n",
    "        filename = year_dir / f\"solar_hourly_generation_{start_year}_{end_year}.csv\"\n",
    "    \n",
    "    if file_exists(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    \n",
    "    directions = {'north': 0, 'east': 90, 'south': 180, 'west': 270}\n",
    "    tilt = round(abs(lat))\n",
    "    \n",
    "    all_data = []\n",
    "    for year in range(start_year - 1, end_year + 1):\n",
    "        year_weather_data = get_weather_data(lat, lon, year)\n",
    "        \n",
    "        year_profiles = {}\n",
    "        for direction, azimuth in directions.items():\n",
    "            year_profiles[direction] = calculate_pv_profile(\n",
    "                year_weather_data, lat, lon, azimuth, tilt, type_pv\n",
    "            )\n",
    "        \n",
    "        year_df = pd.DataFrame({\n",
    "            'timestamp': year_weather_data.index,\n",
    "            'north_solar_capacity_kw_per_kw': year_profiles['north'].values,\n",
    "            'east_solar_capacity_kw_per_kw': year_profiles['east'].values,\n",
    "            'south_solar_capacity_kw_per_kw': year_profiles['south'].values,\n",
    "            'west_solar_capacity_kw_per_kw': year_profiles['west'].values\n",
    "        })\n",
    "        all_data.append(year_df)\n",
    "    \n",
    "    results_df = pd.concat(all_data, ignore_index=True)\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"Created: {filename}\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a9f46b",
   "metadata": {},
   "source": [
    "#### Spot Market Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2675af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_spot_prices(year, month, state='VIC1'):\n",
    "    \"\"\"Download AEMO spot market prices for a given year and month.\"\"\"\n",
    "    filename = year_dir / f'spot_prices_{year}_{month}_{state}.csv'\n",
    "    \n",
    "    if file_exists(filename):\n",
    "        return filename\n",
    "    \n",
    "    url = f'https://www.aemo.com.au/aemo/data/nem/priceanddemand/PRICE_AND_DEMAND_{year}{month}_{state}.csv'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading spot prices: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def download_spot_prices_batch(years, months, state='VIC1', combine_per_year=True):\n",
    "    \"\"\"Download multiple months of spot price data and combine by year.\"\"\"\n",
    "    if isinstance(years, int):\n",
    "        years = [years]\n",
    "    \n",
    "    if not combine_per_year:\n",
    "        downloaded_files = []\n",
    "        for year in years:\n",
    "            for month in months:\n",
    "                file_path = download_spot_prices(year, month, state)\n",
    "                if file_path:\n",
    "                    downloaded_files.append(file_path)\n",
    "        return downloaded_files\n",
    "    \n",
    "    combined_files = []\n",
    "    for year in years:\n",
    "        combined_filename = year_dir / f'spot_prices_{year}_{state}.csv'\n",
    "        \n",
    "        if file_exists(combined_filename):\n",
    "            combined_files.append(combined_filename)\n",
    "            continue\n",
    "        \n",
    "        all_data = []\n",
    "        for month in months:\n",
    "            file_path = download_spot_prices(year, month, state)\n",
    "            if file_path:\n",
    "                df = pd.read_csv(file_path)\n",
    "                all_data.append(df)\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        if all_data:\n",
    "            combined_df = pd.concat(all_data, ignore_index=True)\n",
    "            combined_df.to_csv(combined_filename, index=False)\n",
    "            print(f\"Created: {combined_filename} ({len(combined_df)} records)\")\n",
    "            combined_files.append(combined_filename)\n",
    "    \n",
    "    return combined_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb5bff2",
   "metadata": {},
   "source": [
    "### Data Proccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4a2beb",
   "metadata": {},
   "source": [
    "Generate Synthesized Demand File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "870cac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Excel files found in 'Upload Demand Files Here'\n",
      "\n",
      "Process complete!\n"
     ]
    }
   ],
   "source": [
    "def merge_excel_files_with_electricity_sum(folder_path, output_file=None):\n",
    "    \"\"\"\n",
    "    Merges all Excel files in a folder, combining sheets and files by timestamp,\n",
    "    then sums all numeric columns into a single 'electricity' column.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    folder_path : str\n",
    "        Path to folder containing Excel files\n",
    "    output_file : str, optional\n",
    "        Name of output CSV file (default: auto-generated based on data year range)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Merged dataframe with electricity column\n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Get all Excel files in the folder\n",
    "    excel_files = list(Path(folder_path).glob('*.xlsx')) + list(Path(folder_path).glob('*.xls'))\n",
    "    excel_files = [f for f in excel_files if not f.name.startswith('~$')]\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(f\"No Excel files found in '{folder_path}'\")\n",
    "        return None\n",
    "    \n",
    "    # Process each Excel file\n",
    "    for file_path in excel_files:\n",
    "        print(f\"\\nProcessing: {file_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Read all sheets from the Excel file\n",
    "            excel_file = pd.ExcelFile(file_path)\n",
    "            \n",
    "            for sheet_name in excel_file.sheet_names:\n",
    "                print(f\"  Reading sheet: {sheet_name}\")\n",
    "                \n",
    "                # Read the sheet\n",
    "                df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "                \n",
    "                # Add source information\n",
    "                df['source_file'] = file_path.name\n",
    "                df['source_sheet'] = sheet_name\n",
    "                \n",
    "                all_data.append(df)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {file_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No data was successfully read from the files\")\n",
    "        return None\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    print(\"\\nMerging all data...\")\n",
    "    merged_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"Total rows before processing: {len(merged_df)}\")\n",
    "    print(f\"Data from {len(excel_files)} file(s) and {len(all_data)} sheet(s)\")\n",
    "    \n",
    "    # Identify timestamp column (common names)\n",
    "    timestamp_cols = ['Timestamp', 'timestamp', 'DateTime', 'datetime', 'Date', 'date', 'Time', 'time']\n",
    "    timestamp_col = None\n",
    "    \n",
    "    for col in timestamp_cols:\n",
    "        if col in merged_df.columns:\n",
    "            timestamp_col = col\n",
    "            break\n",
    "    \n",
    "    # Convert timestamp to datetime if not already\n",
    "    if timestamp_col:\n",
    "        print(f\"Found timestamp column: {timestamp_col}\")\n",
    "        # Try to convert to datetime if not already\n",
    "        try:\n",
    "            # First try unix timestamp\n",
    "            if merged_df[timestamp_col].dtype in ['int64', 'float64']:\n",
    "                merged_df[timestamp_col] = pd.to_datetime(merged_df[timestamp_col], unit='s', errors='coerce')\n",
    "            else:\n",
    "                merged_df[timestamp_col] = pd.to_datetime(merged_df[timestamp_col], errors='coerce')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Identify columns to exclude from summation\n",
    "    exclude_cols = ['Hours', 'Timestamp', 'DateTime', 'Date', 'Time', \n",
    "                    'source_file', 'source_sheet', 'Total', 'True Total']\n",
    "    \n",
    "    # Find all numeric columns to sum (excluding the ones we want to keep)\n",
    "    numeric_cols = []\n",
    "    for col in merged_df.columns:\n",
    "        if col not in exclude_cols:\n",
    "            # Check if column is numeric\n",
    "            if pd.api.types.is_numeric_dtype(merged_df[col]):\n",
    "                numeric_cols.append(col)\n",
    "    \n",
    "    print(f\"\\nSumming {len(numeric_cols)} numeric columns into 'electricity' column\")\n",
    "    \n",
    "    # Create electricity column by summing all numeric columns\n",
    "    merged_df['electricity'] = merged_df[numeric_cols].sum(axis=1)\n",
    "    \n",
    "    # Create final dataframe with desired columns\n",
    "    keep_cols = []\n",
    "    \n",
    "    # Keep timestamp-related columns if they exist\n",
    "    for col in ['Hours', 'Timestamp', 'DateTime']:\n",
    "        if col in merged_df.columns:\n",
    "            keep_cols.append(col)\n",
    "    \n",
    "    # Add electricity column\n",
    "    keep_cols.append('electricity')\n",
    "    \n",
    "    final_df = merged_df[keep_cols].copy()\n",
    "    \n",
    "    # Group by timestamp and sum electricity values from all files/sheets\n",
    "    if timestamp_col and timestamp_col in final_df.columns:\n",
    "        print(f\"\\nGrouping by {timestamp_col} and summing electricity values across all files and sheets...\")\n",
    "        \n",
    "        # Create aggregation dictionary\n",
    "        agg_dict = {'electricity': 'sum'}\n",
    "        \n",
    "        # Keep first occurrence of other columns (like Hours, DateTime)\n",
    "        for col in keep_cols:\n",
    "            if col not in [timestamp_col, 'electricity']:\n",
    "                agg_dict[col] = 'first'\n",
    "        \n",
    "        # Group by timestamp and aggregate\n",
    "        final_df = final_df.groupby(timestamp_col, as_index=False).agg(agg_dict)\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        final_df = final_df.sort_values(by=timestamp_col).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Consolidated from {len(merged_df)} total rows to {len(final_df)} unique timestamps\")\n",
    "        \n",
    "        # Determine year range from actual data if output_file not specified\n",
    "        if output_file is None:\n",
    "            try:\n",
    "                first_year = final_df[timestamp_col].min().year\n",
    "                last_year = final_df[timestamp_col].max().year\n",
    "                \n",
    "                if first_year == last_year:\n",
    "                    output_file = f'demand_electricity_{first_year}.csv'\n",
    "                else:\n",
    "                    output_file = f'demand_electricity_{first_year}_{last_year}.csv'\n",
    "                \n",
    "                print(f\"\\nDetected data year range: {first_year} to {last_year}\")\n",
    "            except:\n",
    "                output_file = 'demand_electricity.csv'\n",
    "                print(\"\\nWarning: Could not determine year range, using default filename\")\n",
    "    else:\n",
    "        print(\"\\nWarning: No timestamp column found for grouping. Keeping all rows.\")\n",
    "        if output_file is None:\n",
    "            output_file = 'demand_electricity.csv'\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = Path(folder_path) / output_file\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nMerged data saved to: {output_path}\")\n",
    "    print(f\"Total rows: {len(final_df)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(final_df.head(10))\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Specify your folder path\n",
    "folder_path = 'Upload Demand Files Here' # Change this folder name as needed\n",
    "    \n",
    "# Run the merge function\n",
    "result_df = merge_excel_files_with_electricity_sum(folder_path)\n",
    "    \n",
    "print(\"\\nProcess complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b659a",
   "metadata": {},
   "source": [
    "Data collection and dataframe defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25ae18d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING DATA FILES\n",
      "GENERATING SOLAR PROFILES\n",
      "File already exists: /Users/luellaphillips/eaas1/data/2023/solar_hourly_generation_2023.csv\n",
      "GENERATING RETAIL PRICES\n",
      "Created: /Users/luellaphillips/eaas1/data/2023/grid_prices_2023.csv\n",
      "DOWNLOADING SPOT PRICES\n",
      "File already exists: /Users/luellaphillips/eaas1/data/2023/spot_prices_2023_VIC1.csv\n",
      "RESAMPLING TO 5-MINUTE INTERVALS\n",
      "Detected frequency: h. Resampling to 5min...\n",
      "Created: /Users/luellaphillips/eaas1/data/2023/grid_prices_5min_2023.csv\n",
      "File already exists: /Users/luellaphillips/eaas1/data/2023/solar_generation_5min_2023.csv\n",
      "File already exists: /Users/luellaphillips/eaas1/data/2023/demand_electricity_5min_2023.csv\n",
      "FILE GENERATION COMPLETE!\n",
      "All files saved to: /Users/luellaphillips/eaas1/data/2023\n"
     ]
    }
   ],
   "source": [
    "print(\"GENERATING DATA FILES\")\n",
    "\n",
    "\n",
    "# Generate solar profiles (if not exists)\n",
    "print(\"GENERATING SOLAR PROFILES\")\n",
    "solar_df = generate_directional_solar_profiles(\n",
    "    site_coords[0], site_coords[1], start_year, end_year, pv_type\n",
    ")\n",
    "\n",
    "# Generate retail grid prices (if not exists)\n",
    "print(\"GENERATING RETAIL PRICES\")\n",
    "prices = [[peak_energy_price + oc, off_peak_energy_price + oc] for oc in elec_bill_charges]\n",
    "prices = np.array(prices) / 100  # Convert cents to dollars\n",
    "grid_prices_df = generate_grid_prices(prices, peak_hours, start_year, end_year, start_month)\n",
    "\n",
    "# Download spot prices (optional)\n",
    "if download_spot_prices:\n",
    "    print(\"DOWNLOADING SPOT PRICES\")\n",
    "    years = list(range(start_year, end_year + 1))\n",
    "    months = [f'{i:02d}' for i in range(1, 13)]\n",
    "    spot_files = download_spot_prices_batch(\n",
    "        years=years, months=months, state=nem_state, combine_per_year=True\n",
    "    )\n",
    "\n",
    "# Resample to 5-minute intervals\n",
    "print(\"RESAMPLING TO 5-MINUTE INTERVALS\")\n",
    "\n",
    "# Grid prices to 5-min\n",
    "if start_year == end_year:\n",
    "    grid_5min_path = year_dir / f\"grid_prices_5min_{start_year}.csv\"\n",
    "else:\n",
    "    grid_5min_path = year_dir / f\"grid_prices_5min_{start_year}_{end_year}.csv\"\n",
    "\n",
    "if not file_exists(grid_5min_path):\n",
    "    grid_5min = resample_to_5min(grid_prices_df, 'Datetime')\n",
    "    grid_5min.to_csv(grid_5min_path, index=False)\n",
    "    print(f\"Created: {grid_5min_path}\")\n",
    "\n",
    "# Solar to 5-min\n",
    "if start_year == end_year:\n",
    "    solar_5min_path = year_dir / f\"solar_generation_5min_{start_year}.csv\"\n",
    "else:\n",
    "    solar_5min_path = year_dir / f\"solar_generation_5min_{start_year}_{end_year}.csv\"\n",
    "\n",
    "if not file_exists(solar_5min_path):\n",
    "    solar_df['timestamp'] = pd.to_datetime(solar_df['timestamp'])\n",
    "    solar_df['timestamp'] = solar_df['timestamp'].dt.tz_convert('Australia/Brisbane')\n",
    "    solar_5min = resample_to_5min_kw(\n",
    "        solar_df,\n",
    "        timestamp_col='timestamp',\n",
    "        data_cols=['north_solar_capacity_kw_per_kw', 'east_solar_capacity_kw_per_kw',\n",
    "                  'south_solar_capacity_kw_per_kw', 'west_solar_capacity_kw_per_kw']\n",
    "    )\n",
    "    solar_5min.to_csv(solar_5min_path, index=False)\n",
    "    print(f\"Created: {solar_5min_path}\")\n",
    "\n",
    "# Demand to 5-min (if the hourly file exists in Upload Demand Files Here)\n",
    "demand_hourly_file = Path(data_dir / 'Upload Demand Files Here') / f'demand_electricity_{start_year}.csv'\n",
    "if demand_hourly_file.exists():\n",
    "    if start_year == end_year:\n",
    "        demand_5min_path = year_dir / f\"demand_electricity_5min_{start_year}.csv\"\n",
    "    else:\n",
    "        demand_5min_path = year_dir / f\"demand_electricity_5min_{start_year}_{end_year}.csv\"\n",
    "    \n",
    "    if not file_exists(demand_5min_path):\n",
    "        demand_hourly = pd.read_csv(demand_hourly_file)\n",
    "        demand_5min = resample_to_5min_kw(\n",
    "            demand_hourly, \n",
    "            timestamp_col='Date',\n",
    "            data_cols=['electricity']\n",
    "        )\n",
    "        demand_5min.to_csv(demand_5min_path, index=False)\n",
    "        print(f\"Created: {demand_5min_path}\")\n",
    "\n",
    "\n",
    "print(\"FILE GENERATION COMPLETE!\")\n",
    "print(f\"All files saved to: {year_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e24ab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA COLLECTION AND PROCESSING\n",
      "Output directory: /Users/luellaphillips/eaas1/data/2023\n",
      "Year range: 2023 to 2023\n",
      "LOADING DEMAND DATA\n",
      "Loaded demand data: /Users/luellaphillips/eaas1/data/2023/demand_electricity_5min_2023.csv\n",
      "Shape: (105120, 2)\n",
      "LOADING SPOT PRICES\n",
      "Loaded spot prices: /Users/luellaphillips/eaas1/data/2023/spot_prices_2023_VIC1.csv\n",
      "Shape: (105120, 5)\n",
      "LOADING RETAIL GRID PRICES\n",
      "Loaded grid prices: /Users/luellaphillips/eaas1/data/2023/grid_prices_5min_2023.csv\n",
      "Shape: (105120, 2)\n",
      "LOADING SOLAR GENERATION PROFILES\n",
      "Loaded solar generation: /Users/luellaphillips/eaas1/data/2023/solar_generation_5min_2023.csv\n",
      "Shape: (210240, 5)\n",
      "PROCESSING TIMESTAMPS\n",
      "Demand timestamp column: timestamp\n",
      "Spot prices timestamp column: SETTLEMENTDATE -> timestamp\n",
      "Grid prices timestamp column: Datetime\n",
      "Solar timestamp column: timestamp\n",
      "DATAFRAME SUMMARIES (5-MINUTE INTERVALS)\n",
      "\n",
      "--- Demand DataFrame ---\n",
      "Shape: (105120, 2)\n",
      "Columns: ['timestamp', 'electricity']\n",
      "            timestamp  electricity\n",
      "0 2023-01-01 00:00:00      4.83536\n",
      "1 2023-01-01 00:05:00      4.83536\n",
      "2 2023-01-01 00:10:00      4.83536\n",
      "3 2023-01-01 00:15:00      4.83536\n",
      "4 2023-01-01 00:20:00      4.83536\n",
      "\n",
      "--- Spot Prices DataFrame ---\n",
      "Shape: (105120, 6)\n",
      "Columns: ['REGION', 'SETTLEMENTDATE', 'TOTALDEMAND', 'RRP', 'PERIODTYPE', 'timestamp']\n",
      "  REGION       SETTLEMENTDATE  TOTALDEMAND     RRP PERIODTYPE  \\\n",
      "0   VIC1  2023/01/01 00:05:00      4676.57  129.51      TRADE   \n",
      "1   VIC1  2023/01/01 00:10:00      4722.43  124.72      TRADE   \n",
      "2   VIC1  2023/01/01 00:15:00      4672.63  111.64      TRADE   \n",
      "3   VIC1  2023/01/01 00:20:00      4616.49  110.75      TRADE   \n",
      "4   VIC1  2023/01/01 00:25:00      4604.27  110.91      TRADE   \n",
      "\n",
      "            timestamp  \n",
      "0 2023-01-01 00:05:00  \n",
      "1 2023-01-01 00:10:00  \n",
      "2 2023-01-01 00:15:00  \n",
      "3 2023-01-01 00:20:00  \n",
      "4 2023-01-01 00:25:00  \n",
      "\n",
      "--- Grid Prices DataFrame ---\n",
      "Shape: (105120, 2)\n",
      "Columns: ['Datetime', 'Price ($/kWh)']\n",
      "             Datetime  Price ($/kWh)\n",
      "0 2023-01-01 00:00:00       0.209534\n",
      "1 2023-01-01 00:05:00       0.209534\n",
      "2 2023-01-01 00:10:00       0.209534\n",
      "3 2023-01-01 00:15:00       0.209534\n",
      "4 2023-01-01 00:20:00       0.209534\n",
      "\n",
      "--- Solar DataFrame ---\n",
      "Shape: (210240, 5)\n",
      "Columns: ['timestamp', 'north_solar_capacity_kw_per_kw', 'east_solar_capacity_kw_per_kw', 'south_solar_capacity_kw_per_kw', 'west_solar_capacity_kw_per_kw']\n",
      "            timestamp  north_solar_capacity_kw_per_kw  \\\n",
      "0 2022-01-01 10:00:00                        0.053386   \n",
      "1 2022-01-01 10:05:00                        0.053386   \n",
      "2 2022-01-01 10:10:00                        0.053386   \n",
      "3 2022-01-01 10:15:00                        0.053386   \n",
      "4 2022-01-01 10:20:00                        0.053386   \n",
      "\n",
      "   east_solar_capacity_kw_per_kw  south_solar_capacity_kw_per_kw  \\\n",
      "0                       0.063289                        0.047086   \n",
      "1                       0.063289                        0.047086   \n",
      "2                       0.063289                        0.047086   \n",
      "3                       0.063289                        0.047086   \n",
      "4                       0.063289                        0.047086   \n",
      "\n",
      "   west_solar_capacity_kw_per_kw  \n",
      "0                       0.032261  \n",
      "1                       0.032261  \n",
      "2                       0.032261  \n",
      "3                       0.032261  \n",
      "4                       0.032261  \n",
      "DATA LOADING COMPLETE!\n",
      "All data loaded from: /Users/luellaphillips/eaas1/data/2023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"DATA COLLECTION AND PROCESSING\")\n",
    "print(f\"Output directory: {year_dir}\")\n",
    "print(f\"Year range: {start_year} to {end_year}\")\n",
    "\n",
    "# Initialize all dataframes to None\n",
    "demand_df = None\n",
    "spot_df = None\n",
    "grid_prices_df = None\n",
    "solar_df = None\n",
    "\n",
    "# Load demand data (5-minute intervals)\n",
    "print(\"LOADING DEMAND DATA\")\n",
    "\n",
    "# First, try the expected filename\n",
    "if start_year == end_year:\n",
    "    demand_5min_path = year_dir / f\"demand_electricity_5min_{start_year}.csv\"\n",
    "else:\n",
    "    demand_5min_path = year_dir / f\"demand_electricity_5min_{start_year}_{end_year}.csv\"\n",
    "\n",
    "if demand_5min_path.exists():\n",
    "    demand_df = pd.read_csv(demand_5min_path)\n",
    "    print(f\"Loaded demand data: {demand_5min_path}\")\n",
    "    print(f\"Shape: {demand_df.shape}\")\n",
    "else:\n",
    "    print(f\"Expected file not found: {demand_5min_path}\")\n",
    "    \n",
    "    # Try to find ANY demand 5min file in year_dir\n",
    "    print(\"Searching for any demand_electricity_5min_*.csv file...\")\n",
    "    demand_files = list(year_dir.glob('demand_electricity_5min_*.csv'))\n",
    "    \n",
    "    if demand_files:\n",
    "        demand_df = pd.read_csv(demand_files[0])\n",
    "        print(f\"Loaded demand data from: {demand_files[0]}\")\n",
    "        print(f\"Shape: {demand_df.shape}\")\n",
    "    else:\n",
    "        print(f\"No demand 5min files found in {year_dir}\")\n",
    "        \n",
    "        # Last resort: try Upload Demand Files Here folder\n",
    "        testing_folder = Path('Upload Demand Files Here')\n",
    "        if testing_folder.exists():\n",
    "            print(f\"Searching in {testing_folder}...\")\n",
    "            demand_files = list(testing_folder.glob('demand_electricity_5min_*.csv'))\n",
    "            if demand_files:\n",
    "                demand_df = pd.read_csv(demand_files[0])\n",
    "                print(f\"Loaded demand data from: {demand_files[0]}\")\n",
    "                print(f\"Shape: {demand_df.shape}\")\n",
    "            else:\n",
    "                print(f\"No demand files found in {testing_folder}\")\n",
    "\n",
    "# Load spot prices (5-minute intervals)\n",
    "print(\"LOADING SPOT PRICES\")\n",
    "\n",
    "if start_year == end_year:\n",
    "    spot_file_path = year_dir / f'spot_prices_{start_year}_{nem_state}.csv'\n",
    "else:\n",
    "    spot_file_path = year_dir / f'spot_prices_{start_year}_{end_year}_{nem_state}.csv'\n",
    "\n",
    "if spot_file_path.exists():\n",
    "    spot_df = pd.read_csv(spot_file_path)\n",
    "    print(f\"Loaded spot prices: {spot_file_path}\")\n",
    "    print(f\"Shape: {spot_df.shape}\")\n",
    "else:\n",
    "    print(f\"Spot price file not found: {spot_file_path}\")\n",
    "\n",
    "# Load retail grid prices (5-minute intervals)\n",
    "print(\"LOADING RETAIL GRID PRICES\")\n",
    "\n",
    "if start_year == end_year:\n",
    "    grid_5min_path = year_dir / f\"grid_prices_5min_{start_year}.csv\"\n",
    "else:\n",
    "    grid_5min_path = year_dir / f\"grid_prices_5min_{start_year}_{end_year}.csv\"\n",
    "\n",
    "if grid_5min_path.exists():\n",
    "    grid_prices_df = pd.read_csv(grid_5min_path)\n",
    "    print(f\"Loaded grid prices: {grid_5min_path}\")\n",
    "    print(f\"Shape: {grid_prices_df.shape}\")\n",
    "else:\n",
    "    print(f\"Grid price file not found: {grid_5min_path}\")\n",
    "\n",
    "# Load solar generation profiles (5-minute intervals)\n",
    "print(\"LOADING SOLAR GENERATION PROFILES\")\n",
    "\n",
    "if start_year == end_year:\n",
    "    solar_5min_path = year_dir / f\"solar_generation_5min_{start_year}.csv\"\n",
    "else:\n",
    "    solar_5min_path = year_dir / f\"solar_generation_5min_{start_year}_{end_year}.csv\"\n",
    "\n",
    "if solar_5min_path.exists():\n",
    "    solar_df = pd.read_csv(solar_5min_path)\n",
    "    print(f\"Loaded solar generation: {solar_5min_path}\")\n",
    "    print(f\"Shape: {solar_df.shape}\")\n",
    "else:\n",
    "    print(f\"Solar generation file not found: {solar_5min_path}\")\n",
    "\n",
    "# Convert timestamp columns to datetime for all dataframes\n",
    "print(\"PROCESSING TIMESTAMPS\")\n",
    "\n",
    "if demand_df is not None:\n",
    "    timestamp_col = None\n",
    "    for col in ['timestamp', 'Timestamp', 'DateTime', 'datetime']:\n",
    "        if col in demand_df.columns:\n",
    "            timestamp_col = col\n",
    "            break\n",
    "    if timestamp_col:\n",
    "        #demand_df[timestamp_col] = pd.to_datetime(demand_df[timestamp_col], format=\"%d/%m/%Y %H:%M\")\n",
    "        demand_df[timestamp_col] = pd.to_datetime(demand_df[timestamp_col])\n",
    "        print(f\"Demand timestamp column: {timestamp_col}\")\n",
    "\n",
    "if spot_df is not None:\n",
    "    if 'SETTLEMENTDATE' in spot_df.columns:\n",
    "        spot_df['timestamp'] = pd.to_datetime(spot_df['SETTLEMENTDATE'])\n",
    "        print(\"Spot prices timestamp column: SETTLEMENTDATE -> timestamp\")\n",
    "    elif 'timestamp' in spot_df.columns:\n",
    "        spot_df['timestamp'] = pd.to_datetime(spot_df['timestamp'])\n",
    "        print(\"Spot prices timestamp column: timestamp\")\n",
    "\n",
    "if grid_prices_df is not None:\n",
    "    if 'Datetime' in grid_prices_df.columns:\n",
    "        grid_prices_df['Datetime'] = pd.to_datetime(grid_prices_df['Datetime'])\n",
    "        print(\"Grid prices timestamp column: Datetime\")\n",
    "    elif 'timestamp' in grid_prices_df.columns:\n",
    "        grid_prices_df['timestamp'] = pd.to_datetime(grid_prices_df['timestamp'])\n",
    "        print(\"Grid prices timestamp column: timestamp\")\n",
    "\n",
    "\n",
    "if solar_df is not None:\n",
    "    if 'timestamp' in solar_df.columns:\n",
    "        #solar_df['timestamp'] = pd.to_datetime(solar_df['timestamp'], format=\"%d/%m/%Y %H:%M\").dt.tz_localize(None)\n",
    "        solar_df['timestamp'] = pd.to_datetime(solar_df['timestamp']).dt.tz_localize(None)\n",
    "        print(\"Solar timestamp column: timestamp\")\n",
    "\n",
    "# Display dataframe summaries\n",
    "print(\"DATAFRAME SUMMARIES (5-MINUTE INTERVALS)\")\n",
    "\n",
    "print(\"\\n--- Demand DataFrame ---\")\n",
    "if demand_df is not None:\n",
    "    print(f\"Shape: {demand_df.shape}\")\n",
    "    print(f\"Columns: {list(demand_df.columns)}\")\n",
    "    print(demand_df.head())\n",
    "else:\n",
    "    print(\"Not loaded\")\n",
    "\n",
    "print(\"\\n--- Spot Prices DataFrame ---\")\n",
    "if spot_df is not None:\n",
    "    print(f\"Shape: {spot_df.shape}\")\n",
    "    print(f\"Columns: {list(spot_df.columns)}\")\n",
    "    print(spot_df.head())\n",
    "else:\n",
    "    print(\"Not loaded\")\n",
    "\n",
    "print(\"\\n--- Grid Prices DataFrame ---\")\n",
    "if grid_prices_df is not None:\n",
    "    print(f\"Shape: {grid_prices_df.shape}\")\n",
    "    print(f\"Columns: {list(grid_prices_df.columns)}\")\n",
    "    print(grid_prices_df.head())\n",
    "else:\n",
    "    print(\"Not loaded\")\n",
    "\n",
    "print(\"\\n--- Solar DataFrame ---\")\n",
    "if solar_df is not None:\n",
    "    print(f\"Shape: {solar_df.shape}\")\n",
    "    print(f\"Columns: {list(solar_df.columns)}\")\n",
    "    print(solar_df.head())\n",
    "else:\n",
    "    print(\"Not loaded\")\n",
    "\n",
    "print(\"DATA LOADING COMPLETE!\")\n",
    "print(f\"All data loaded from: {year_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c96a1",
   "metadata": {},
   "source": [
    "Allign dataframes by set year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b398069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALIGNING DATAFRAMES TO YEAR 2023\n",
      "\n",
      "Reference timestamp range:\n",
      "  Start: 2023-01-01 00:00:00\n",
      "  End: 2023-12-31 23:55:00\n",
      "  Total intervals: 105120\n",
      "\n",
      "Aligning demand data...\n",
      "  Original shape: (105120, 2)\n",
      "  Filtered to 2023: (105120, 2)\n",
      "  Aligned shape: (105120, 2)\n",
      "  Missing values: 0\n",
      "\n",
      "Aligning spot price data...\n",
      "  Filled first timestamp with next available value: 0.12951\n",
      "  Original shape: (105120, 6)\n",
      "  Filtered to 2023: (105119, 6)\n",
      "  Aligned shape: (105120, 2)\n",
      "  Missing values: 0\n",
      "\n",
      "Aligning grid price data...\n",
      "  Original shape: (105120, 2)\n",
      "  Filtered to 2023: (105120, 3)\n",
      "  Aligned shape: (105120, 2)\n",
      "  Missing values: 0\n",
      "\n",
      "Aligning solar generation data...\n",
      "  Filled missing values in north_solar_capacity_kw_per_kw with 0\n",
      "  Filled missing values in east_solar_capacity_kw_per_kw with 0\n",
      "  Filled missing values in south_solar_capacity_kw_per_kw with 0\n",
      "  Filled missing values in west_solar_capacity_kw_per_kw with 0\n",
      "  Original shape: (210240, 5)\n",
      "  Filtered to 2023: (105120, 5)\n",
      "  Aligned shape: (105120, 5)\n",
      "  Solar columns: ['north_solar_capacity_kw_per_kw', 'east_solar_capacity_kw_per_kw', 'south_solar_capacity_kw_per_kw', 'west_solar_capacity_kw_per_kw']\n",
      "  Missing values in north_solar_capacity_kw_per_kw: 0\n",
      "  Missing values in east_solar_capacity_kw_per_kw: 0\n",
      "  Missing values in south_solar_capacity_kw_per_kw: 0\n",
      "  Missing values in west_solar_capacity_kw_per_kw: 0\n",
      "\n",
      "================================================================================\n",
      "ALIGNMENT SUMMARY\n",
      "================================================================================\n",
      "Target year: 2023\n",
      "Reference intervals: 105120\n",
      "\n",
      "Aligned dataframe shapes:\n",
      "  Demand: (105120, 2)\n",
      "  Spot prices: (105120, 2)\n",
      "  Grid prices: (105120, 2)\n",
      "  Solar generation: (105120, 5)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ALIGNED DATAFRAMES\n",
      "================================================================================\n",
      "\n",
      "--- Demand DataFrame ---\n",
      "            timestamp  electricity\n",
      "0 2023-01-01 00:00:00      4.83536\n",
      "1 2023-01-01 00:05:00      4.83536\n",
      "2 2023-01-01 00:10:00      4.83536\n",
      "3 2023-01-01 00:15:00      4.83536\n",
      "4 2023-01-01 00:20:00      4.83536\n",
      "5 2023-01-01 00:25:00      4.83536\n",
      "6 2023-01-01 00:30:00      4.83536\n",
      "7 2023-01-01 00:35:00      4.83536\n",
      "8 2023-01-01 00:40:00      4.83536\n",
      "9 2023-01-01 00:45:00      4.83536\n",
      "\n",
      "Data range: 2023-01-01 00:00:00 to 2023-12-31 23:55:00\n",
      "Shape: (105120, 2)\n",
      "\n",
      "--- Spot Prices DataFrame ---\n",
      "            timestamp  spot_price_kwh\n",
      "0 2023-01-01 00:00:00         0.12951\n",
      "1 2023-01-01 00:05:00         0.12951\n",
      "2 2023-01-01 00:10:00         0.12472\n",
      "3 2023-01-01 00:15:00         0.11164\n",
      "4 2023-01-01 00:20:00         0.11075\n",
      "5 2023-01-01 00:25:00         0.11091\n",
      "6 2023-01-01 00:30:00         0.09930\n",
      "7 2023-01-01 00:35:00         0.10888\n",
      "8 2023-01-01 00:40:00         0.10693\n",
      "9 2023-01-01 00:45:00         0.09908\n",
      "\n",
      "Data range: 2023-01-01 00:00:00 to 2023-12-31 23:55:00\n",
      "Shape: (105120, 2)\n",
      "\n",
      "--- Grid Prices DataFrame ---\n",
      "            timestamp  retail_price_kwh\n",
      "0 2023-01-01 00:00:00          0.209534\n",
      "1 2023-01-01 00:05:00          0.209534\n",
      "2 2023-01-01 00:10:00          0.209534\n",
      "3 2023-01-01 00:15:00          0.209534\n",
      "4 2023-01-01 00:20:00          0.209534\n",
      "5 2023-01-01 00:25:00          0.209534\n",
      "6 2023-01-01 00:30:00          0.209534\n",
      "7 2023-01-01 00:35:00          0.209534\n",
      "8 2023-01-01 00:40:00          0.209534\n",
      "9 2023-01-01 00:45:00          0.209534\n",
      "\n",
      "Data range: 2023-01-01 00:00:00 to 2023-12-31 23:55:00\n",
      "Shape: (105120, 2)\n",
      "\n",
      "--- Solar Generation DataFrame ---\n",
      "            timestamp  north_solar_capacity_kw_per_kw  \\\n",
      "0 2023-01-01 00:00:00                             0.0   \n",
      "1 2023-01-01 00:05:00                             0.0   \n",
      "2 2023-01-01 00:10:00                             0.0   \n",
      "3 2023-01-01 00:15:00                             0.0   \n",
      "4 2023-01-01 00:20:00                             0.0   \n",
      "5 2023-01-01 00:25:00                             0.0   \n",
      "6 2023-01-01 00:30:00                             0.0   \n",
      "7 2023-01-01 00:35:00                             0.0   \n",
      "8 2023-01-01 00:40:00                             0.0   \n",
      "9 2023-01-01 00:45:00                             0.0   \n",
      "\n",
      "   east_solar_capacity_kw_per_kw  south_solar_capacity_kw_per_kw  \\\n",
      "0                            0.0                             0.0   \n",
      "1                            0.0                             0.0   \n",
      "2                            0.0                             0.0   \n",
      "3                            0.0                             0.0   \n",
      "4                            0.0                             0.0   \n",
      "5                            0.0                             0.0   \n",
      "6                            0.0                             0.0   \n",
      "7                            0.0                             0.0   \n",
      "8                            0.0                             0.0   \n",
      "9                            0.0                             0.0   \n",
      "\n",
      "   west_solar_capacity_kw_per_kw  \n",
      "0                            0.0  \n",
      "1                            0.0  \n",
      "2                            0.0  \n",
      "3                            0.0  \n",
      "4                            0.0  \n",
      "5                            0.0  \n",
      "6                            0.0  \n",
      "7                            0.0  \n",
      "8                            0.0  \n",
      "9                            0.0  \n",
      "\n",
      "Data range: 2023-01-01 00:00:00 to 2023-12-31 23:55:00\n",
      "Shape: (105120, 5)\n",
      "ALIGNMENT COMPLETE!\n",
      "All dataframes have been aligned and replaced with aligned versions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def align_dataframes_by_year(demand_df, spot_df, grid_prices_df, solar_df, target_year):\n",
    "    \"\"\"\n",
    "    Align all dataframes to a specific year with matching timestamps at 5-minute intervals.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    demand_df : pd.DataFrame\n",
    "        Demand data with timestamp column\n",
    "    spot_df : pd.DataFrame\n",
    "        Spot price data with timestamp column\n",
    "    grid_prices_df : pd.DataFrame\n",
    "        Grid/retail price data with timestamp column\n",
    "    solar_df : pd.DataFrame\n",
    "        Solar generation data with timestamp column\n",
    "    target_year : int\n",
    "        Year to align all dataframes to\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of pd.DataFrame\n",
    "        (aligned_demand, aligned_spot, aligned_grid, aligned_solar)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ALIGNING DATAFRAMES TO YEAR {target_year}\")\n",
    "    \n",
    "    # Create reference timestamp range for the target year (5-minute intervals)\n",
    "    start_date = pd.Timestamp(f'{target_year}-01-01 00:00:00')\n",
    "    end_date = pd.Timestamp(f'{target_year}-12-31 23:55:00')\n",
    "    reference_timestamps = pd.date_range(start=start_date, end=end_date, freq='5min')\n",
    "    \n",
    "    print(f\"\\nReference timestamp range:\")\n",
    "    print(f\"  Start: {reference_timestamps[0]}\")\n",
    "    print(f\"  End: {reference_timestamps[-1]}\")\n",
    "    print(f\"  Total intervals: {len(reference_timestamps)}\")\n",
    "    \n",
    "    # Create reference dataframe\n",
    "    aligned_data = pd.DataFrame({'timestamp': reference_timestamps})\n",
    "    \n",
    "    # Function to identify timestamp column\n",
    "    def find_timestamp_col(df):\n",
    "        for col in ['timestamp', 'Timestamp', 'DateTime', 'Datetime', 'datetime', 'SETTLEMENTDATE']:\n",
    "            if col in df.columns:\n",
    "                return col\n",
    "        return None\n",
    "    \n",
    "    # Function to remove timezone info\n",
    "    def remove_timezone(timestamp_series):\n",
    "        \"\"\"Remove timezone information from timestamp series\"\"\"\n",
    "        if hasattr(timestamp_series.dtype, 'tz') and timestamp_series.dtype.tz is not None:\n",
    "            return timestamp_series.dt.tz_localize(None)\n",
    "        return timestamp_series\n",
    "    \n",
    "    # Align demand data\n",
    "    if demand_df is not None:\n",
    "        print(\"\\nAligning demand data...\")\n",
    "        ts_col = find_timestamp_col(demand_df)\n",
    "        if ts_col:\n",
    "            # Ensure timestamp is datetime\n",
    "            demand_df = demand_df.copy()\n",
    "            demand_df[ts_col] = pd.to_datetime(demand_df[ts_col])\n",
    "            demand_df[ts_col] = remove_timezone(demand_df[ts_col])\n",
    "            \n",
    "            # Filter to target year\n",
    "            demand_year = demand_df[demand_df[ts_col].dt.year == target_year].copy()\n",
    "            demand_year['timestamp'] = demand_year[ts_col]\n",
    "            \n",
    "            # Merge with reference timestamps\n",
    "            aligned_demand = aligned_data.merge(\n",
    "                demand_year[['timestamp', 'electricity']], \n",
    "                on='timestamp', \n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            print(f\"  Original shape: {demand_df.shape}\")\n",
    "            print(f\"  Filtered to {target_year}: {demand_year.shape}\")\n",
    "            print(f\"  Aligned shape: {aligned_demand.shape}\")\n",
    "            print(f\"  Missing values: {aligned_demand['electricity'].isna().sum()}\")\n",
    "        else:\n",
    "            print(\"  Warning: No timestamp column found\")\n",
    "            aligned_demand = aligned_data.copy()\n",
    "            aligned_demand['electricity'] = np.nan\n",
    "    else:\n",
    "        print(\"\\nDemand data not available\")\n",
    "        aligned_demand = aligned_data.copy()\n",
    "        aligned_demand['electricity'] = np.nan\n",
    "    \n",
    "    # Align spot price data\n",
    "    if spot_df is not None:\n",
    "        print(\"\\nAligning spot price data...\")\n",
    "        ts_col = find_timestamp_col(spot_df)\n",
    "        if ts_col:\n",
    "            spot_df = spot_df.copy()\n",
    "            spot_df[ts_col] = pd.to_datetime(spot_df[ts_col])\n",
    "            spot_df[ts_col] = remove_timezone(spot_df[ts_col])\n",
    "            \n",
    "            # Filter to target year\n",
    "            spot_year = spot_df[spot_df[ts_col].dt.year == target_year].copy()\n",
    "            spot_year['timestamp'] = spot_year[ts_col]\n",
    "            \n",
    "            # Get RRP column (spot price) - check multiple possible names\n",
    "            if 'RRP' in spot_year.columns:\n",
    "                price_col = 'RRP'\n",
    "            elif 'price_per_kwh' in spot_year.columns:\n",
    "                price_col = 'price_per_kwh'\n",
    "            elif 'spot_price_kwh' in spot_year.columns:\n",
    "                price_col = 'spot_price_kwh'\n",
    "            else:\n",
    "                # Find any column with 'price' in the name\n",
    "                price_cols = [col for col in spot_year.columns if 'price' in col.lower() or 'rrp' in col.lower()]\n",
    "                if price_cols:\n",
    "                    price_col = price_cols[0]\n",
    "                    print(f\"  Using price column: {price_col}\")\n",
    "                else:\n",
    "                    print(f\"  Warning: No price column found. Available columns: {list(spot_year.columns)}\")\n",
    "                    aligned_spot = aligned_data.copy()\n",
    "                    aligned_spot['spot_price_kwh'] = np.nan\n",
    "                    price_col = None\n",
    "            \n",
    "            if price_col:\n",
    "                # Merge with reference timestamps\n",
    "                aligned_spot = aligned_data.merge(\n",
    "                    spot_year[['timestamp', price_col]], \n",
    "                    on='timestamp', \n",
    "                    how='left'\n",
    "                )\n",
    "                \n",
    "                # Convert RRP from $/MWh to $/kWh if needed\n",
    "                if price_col == 'RRP':\n",
    "                    aligned_spot['spot_price_kwh'] = aligned_spot['RRP'] / 1000\n",
    "                    aligned_spot = aligned_spot.drop('RRP', axis=1)\n",
    "                else:\n",
    "                    # Assume it's already in $/kWh or rename it\n",
    "                    aligned_spot['spot_price_kwh'] = aligned_spot[price_col]\n",
    "                    if price_col != 'spot_price_kwh':\n",
    "                        aligned_spot = aligned_spot.drop(price_col, axis=1)\n",
    "                \n",
    "                # Fill the first timestamp's missing value with the next available value\n",
    "                if pd.isna(aligned_spot.loc[0, 'spot_price_kwh']):\n",
    "                    first_valid_value = aligned_spot['spot_price_kwh'].dropna().iloc[0]\n",
    "                    aligned_spot.loc[0, 'spot_price_kwh'] = first_valid_value\n",
    "                    print(f\"  Filled first timestamp with next available value: {first_valid_value}\")\n",
    "                \n",
    "                print(f\"  Original shape: {spot_df.shape}\")\n",
    "                print(f\"  Filtered to {target_year}: {spot_year.shape}\")\n",
    "                print(f\"  Aligned shape: {aligned_spot.shape}\")\n",
    "                print(f\"  Missing values: {aligned_spot['spot_price_kwh'].isna().sum()}\")\n",
    "        else:\n",
    "            print(\"  Warning: No timestamp column found\")\n",
    "            aligned_spot = aligned_data.copy()\n",
    "            aligned_spot['spot_price_kwh'] = np.nan\n",
    "    else:\n",
    "        print(\"\\nSpot price data not available\")\n",
    "        aligned_spot = aligned_data.copy()\n",
    "        aligned_spot['spot_price_kwh'] = np.nan\n",
    "    \n",
    "    # Align grid price data\n",
    "    if grid_prices_df is not None:\n",
    "        print(\"\\nAligning grid price data...\")\n",
    "        ts_col = find_timestamp_col(grid_prices_df)\n",
    "        if ts_col:\n",
    "            grid_prices_df = grid_prices_df.copy()\n",
    "            grid_prices_df[ts_col] = pd.to_datetime(grid_prices_df[ts_col])\n",
    "            grid_prices_df[ts_col] = remove_timezone(grid_prices_df[ts_col])\n",
    "            \n",
    "            # Filter to target year\n",
    "            grid_year = grid_prices_df[grid_prices_df[ts_col].dt.year == target_year].copy()\n",
    "            grid_year['timestamp'] = grid_year[ts_col]\n",
    "            \n",
    "            # Get price column - check multiple possible names\n",
    "            if 'Price ($/kWh)' in grid_year.columns:\n",
    "                price_col = 'Price ($/kWh)'\n",
    "            elif 'price' in grid_year.columns:\n",
    "                price_col = 'price'\n",
    "            elif 'retail_price_kwh' in grid_year.columns:\n",
    "                price_col = 'retail_price_kwh'\n",
    "            else:\n",
    "                # Find any column with 'price' in the name\n",
    "                price_cols = [col for col in grid_year.columns if 'price' in col.lower()]\n",
    "                if price_cols:\n",
    "                    price_col = price_cols[0]\n",
    "                    print(f\"  Using price column: {price_col}\")\n",
    "                else:\n",
    "                    print(f\"  Warning: No price column found. Available columns: {list(grid_year.columns)}\")\n",
    "                    aligned_grid = aligned_data.copy()\n",
    "                    aligned_grid['retail_price_kwh'] = np.nan\n",
    "                    price_col = None\n",
    "            \n",
    "            if price_col:\n",
    "                # Merge with reference timestamps\n",
    "                aligned_grid = aligned_data.merge(\n",
    "                    grid_year[['timestamp', price_col]], \n",
    "                    on='timestamp', \n",
    "                    how='left'\n",
    "                )\n",
    "                \n",
    "                # Rename to standard column name\n",
    "                if price_col != 'retail_price_kwh':\n",
    "                    aligned_grid = aligned_grid.rename(columns={price_col: 'retail_price_kwh'})\n",
    "                \n",
    "                print(f\"  Original shape: {grid_prices_df.shape}\")\n",
    "                print(f\"  Filtered to {target_year}: {grid_year.shape}\")\n",
    "                print(f\"  Aligned shape: {aligned_grid.shape}\")\n",
    "                print(f\"  Missing values: {aligned_grid['retail_price_kwh'].isna().sum()}\")\n",
    "        else:\n",
    "            print(\"  Warning: No timestamp column found\")\n",
    "            aligned_grid = aligned_data.copy()\n",
    "            aligned_grid['retail_price_kwh'] = np.nan\n",
    "    else:\n",
    "        print(\"\\nGrid price data not available\")\n",
    "        aligned_grid = aligned_data.copy()\n",
    "        aligned_grid['retail_price_kwh'] = np.nan\n",
    "    \n",
    "    # Align solar data\n",
    "    if solar_df is not None:\n",
    "        print(\"\\nAligning solar generation data...\")\n",
    "        ts_col = find_timestamp_col(solar_df)\n",
    "        if ts_col:\n",
    "            solar_df = solar_df.copy()\n",
    "            solar_df[ts_col] = pd.to_datetime(solar_df[ts_col])\n",
    "            solar_df[ts_col] = remove_timezone(solar_df[ts_col])\n",
    "            \n",
    "            # Filter to target year\n",
    "            solar_year = solar_df[solar_df[ts_col].dt.year == target_year].copy()\n",
    "            solar_year['timestamp'] = solar_year[ts_col]\n",
    "            \n",
    "            # Get solar columns\n",
    "            solar_cols = ['north_solar_capacity_kw_per_kw', 'east_solar_capacity_kw_per_kw',\n",
    "                         'south_solar_capacity_kw_per_kw', 'west_solar_capacity_kw_per_kw']\n",
    "            available_cols = [col for col in solar_cols if col in solar_year.columns]\n",
    "            \n",
    "            if available_cols:\n",
    "                # Merge with reference timestamps\n",
    "                aligned_solar = aligned_data.merge(\n",
    "                    solar_year[['timestamp'] + available_cols], \n",
    "                    on='timestamp', \n",
    "                    how='left'\n",
    "                )\n",
    "                \n",
    "                # Fill missing values with 0 for solar columns\n",
    "                for col in available_cols:\n",
    "                    aligned_solar[col] = aligned_solar[col].fillna(0)\n",
    "                    print(f\"  Filled missing values in {col} with 0\")\n",
    "                \n",
    "                print(f\"  Original shape: {solar_df.shape}\")\n",
    "                print(f\"  Filtered to {target_year}: {solar_year.shape}\")\n",
    "                print(f\"  Aligned shape: {aligned_solar.shape}\")\n",
    "                print(f\"  Solar columns: {available_cols}\")\n",
    "                for col in available_cols:\n",
    "                    print(f\"  Missing values in {col}: {aligned_solar[col].isna().sum()}\")\n",
    "            else:\n",
    "                print(\"  Warning: No solar capacity columns found\")\n",
    "                aligned_solar = aligned_data.copy()\n",
    "        else:\n",
    "            print(\"  Warning: No timestamp column found\")\n",
    "            aligned_solar = aligned_data.copy()\n",
    "    else:\n",
    "        print(\"\\nSolar data not available\")\n",
    "        aligned_solar = aligned_data.copy()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ALIGNMENT SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Target year: {target_year}\")\n",
    "    print(f\"Reference intervals: {len(reference_timestamps)}\")\n",
    "    print(f\"\\nAligned dataframe shapes:\")\n",
    "    print(f\"  Demand: {aligned_demand.shape}\")\n",
    "    print(f\"  Spot prices: {aligned_spot.shape}\")\n",
    "    print(f\"  Grid prices: {aligned_grid.shape}\")\n",
    "    print(f\"  Solar generation: {aligned_solar.shape}\")\n",
    "    \n",
    "    return aligned_demand, aligned_spot, aligned_grid, aligned_solar\n",
    "\n",
    "\n",
    "# Specify the target year for alignment\n",
    "alignment_year = start_year  # Use the configured start_year, or set manually\n",
    "\n",
    "# Run alignment and replace original dataframes\n",
    "demand_df, spot_df, grid_prices_df, solar_df = align_dataframes_by_year(\n",
    "    demand_df, spot_df, grid_prices_df, solar_df, alignment_year\n",
    ")\n",
    "\n",
    "# Display aligned data\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"ALIGNED DATAFRAMES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- Demand DataFrame ---\")\n",
    "print(demand_df.head(10))\n",
    "print(f\"\\nData range: {demand_df['timestamp'].min()} to {demand_df['timestamp'].max()}\")\n",
    "print(f\"Shape: {demand_df.shape}\")\n",
    "\n",
    "print(\"\\n--- Spot Prices DataFrame ---\")\n",
    "print(spot_df.head(10))\n",
    "print(f\"\\nData range: {spot_df['timestamp'].min()} to {spot_df['timestamp'].max()}\")\n",
    "print(f\"Shape: {spot_df.shape}\")\n",
    "\n",
    "print(\"\\n--- Grid Prices DataFrame ---\")\n",
    "print(grid_prices_df.head(10))\n",
    "print(f\"\\nData range: {grid_prices_df['timestamp'].min()} to {grid_prices_df['timestamp'].max()}\")\n",
    "print(f\"Shape: {grid_prices_df.shape}\")\n",
    "\n",
    "print(\"\\n--- Solar Generation DataFrame ---\")\n",
    "print(solar_df.head(10))\n",
    "print(f\"\\nData range: {solar_df['timestamp'].min()} to {solar_df['timestamp'].max()}\")\n",
    "print(f\"Shape: {solar_df.shape}\")\n",
    "\n",
    "print(\"ALIGNMENT COMPLETE!\")\n",
    "print(\"All dataframes have been aligned and replaced with aligned versions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
